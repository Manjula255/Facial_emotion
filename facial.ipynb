{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "yVU283IFHAea",
        "outputId": "3ef5a4d8-8c66-4e92-e81a-4714aa15e5bb"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c9f7fab5-0ce9-4e01-b2a1-f02de1654276\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c9f7fab5-0ce9-4e01-b2a1-f02de1654276\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving archive.zip to archive.zip\n",
            "User uploaded file \"archive.zip\" with length 63252113 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldKATCs0Hn9t",
        "outputId": "68a97561-aa73-47da-d2b5-e89361492c9a"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"archive.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Cl24XZHtT7"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMKGffdHH8IT",
        "outputId": "46b47f12-2cc5-482f-dffb-5d1ab30aaa01"
      },
      "source": [
        "train_dir = 'train'\n",
        "val_dir = 'test'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx_PTi0rH8Uy"
      },
      "source": [
        "emotion_model = Sequential()\n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "emotion_model.add(Flatten())\n",
        "emotion_model.add(Dense(1024, activation='relu'))\n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olmMnVu_H8ei",
        "outputId": "4cc35e99-7806-4183-a826-cb92e51a4ff0"
      },
      "source": [
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "emotion_model_info = emotion_model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=28709 // 64,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=7178 // 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "448/448 [==============================] - 29s 45ms/step - loss: 1.8003 - accuracy: 0.2607 - val_loss: 1.7252 - val_accuracy: 0.3383\n",
            "Epoch 2/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 1.6313 - accuracy: 0.3643 - val_loss: 1.5472 - val_accuracy: 0.4121\n",
            "Epoch 3/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 1.5272 - accuracy: 0.4167 - val_loss: 1.4539 - val_accuracy: 0.4448\n",
            "Epoch 4/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.4591 - accuracy: 0.4400 - val_loss: 1.3949 - val_accuracy: 0.4643\n",
            "Epoch 5/50\n",
            "448/448 [==============================] - 18s 39ms/step - loss: 1.3923 - accuracy: 0.4701 - val_loss: 1.3391 - val_accuracy: 0.4937\n",
            "Epoch 6/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.3396 - accuracy: 0.4931 - val_loss: 1.2954 - val_accuracy: 0.5067\n",
            "Epoch 7/50\n",
            "448/448 [==============================] - 18s 39ms/step - loss: 1.2931 - accuracy: 0.5113 - val_loss: 1.2620 - val_accuracy: 0.5212\n",
            "Epoch 8/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.2604 - accuracy: 0.5235 - val_loss: 1.2365 - val_accuracy: 0.5271\n",
            "Epoch 9/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.2246 - accuracy: 0.5383 - val_loss: 1.2281 - val_accuracy: 0.5335\n",
            "Epoch 10/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.1947 - accuracy: 0.5493 - val_loss: 1.1952 - val_accuracy: 0.5467\n",
            "Epoch 11/50\n",
            "448/448 [==============================] - 18s 39ms/step - loss: 1.1677 - accuracy: 0.5628 - val_loss: 1.1735 - val_accuracy: 0.5543\n",
            "Epoch 12/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.1406 - accuracy: 0.5730 - val_loss: 1.1549 - val_accuracy: 0.5618\n",
            "Epoch 13/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.1157 - accuracy: 0.5843 - val_loss: 1.1439 - val_accuracy: 0.5671\n",
            "Epoch 14/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.0916 - accuracy: 0.5922 - val_loss: 1.1377 - val_accuracy: 0.5681\n",
            "Epoch 15/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.0709 - accuracy: 0.6008 - val_loss: 1.1228 - val_accuracy: 0.5790\n",
            "Epoch 16/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.0469 - accuracy: 0.6107 - val_loss: 1.1100 - val_accuracy: 0.5829\n",
            "Epoch 17/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.0199 - accuracy: 0.6193 - val_loss: 1.0963 - val_accuracy: 0.5928\n",
            "Epoch 18/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 1.0011 - accuracy: 0.6308 - val_loss: 1.0964 - val_accuracy: 0.5869\n",
            "Epoch 19/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.9783 - accuracy: 0.6367 - val_loss: 1.0900 - val_accuracy: 0.5897\n",
            "Epoch 20/50\n",
            "448/448 [==============================] - 18s 39ms/step - loss: 0.9594 - accuracy: 0.6426 - val_loss: 1.0828 - val_accuracy: 0.6010\n",
            "Epoch 21/50\n",
            "448/448 [==============================] - 18s 39ms/step - loss: 0.9375 - accuracy: 0.6535 - val_loss: 1.0882 - val_accuracy: 0.5932\n",
            "Epoch 22/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.9159 - accuracy: 0.6614 - val_loss: 1.0736 - val_accuracy: 0.6052\n",
            "Epoch 23/50\n",
            "448/448 [==============================] - 18s 40ms/step - loss: 0.8938 - accuracy: 0.6680 - val_loss: 1.0725 - val_accuracy: 0.6034\n",
            "Epoch 24/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.8695 - accuracy: 0.6796 - val_loss: 1.0564 - val_accuracy: 0.6098\n",
            "Epoch 25/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.8547 - accuracy: 0.6833 - val_loss: 1.0682 - val_accuracy: 0.6087\n",
            "Epoch 26/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.8305 - accuracy: 0.6949 - val_loss: 1.0620 - val_accuracy: 0.6078\n",
            "Epoch 27/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.8098 - accuracy: 0.7004 - val_loss: 1.0756 - val_accuracy: 0.6172\n",
            "Epoch 28/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.7896 - accuracy: 0.7117 - val_loss: 1.0593 - val_accuracy: 0.6172\n",
            "Epoch 29/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.7677 - accuracy: 0.7184 - val_loss: 1.0597 - val_accuracy: 0.6208\n",
            "Epoch 30/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.7466 - accuracy: 0.7262 - val_loss: 1.0568 - val_accuracy: 0.6230\n",
            "Epoch 31/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.7230 - accuracy: 0.7349 - val_loss: 1.0632 - val_accuracy: 0.6203\n",
            "Epoch 32/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.6992 - accuracy: 0.7452 - val_loss: 1.0713 - val_accuracy: 0.6189\n",
            "Epoch 33/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.6821 - accuracy: 0.7505 - val_loss: 1.0665 - val_accuracy: 0.6208\n",
            "Epoch 34/50\n",
            "448/448 [==============================] - 21s 46ms/step - loss: 0.6569 - accuracy: 0.7613 - val_loss: 1.0936 - val_accuracy: 0.6196\n",
            "Epoch 35/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.6438 - accuracy: 0.7645 - val_loss: 1.0855 - val_accuracy: 0.6242\n",
            "Epoch 36/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.6209 - accuracy: 0.7718 - val_loss: 1.0804 - val_accuracy: 0.6247\n",
            "Epoch 37/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.5987 - accuracy: 0.7815 - val_loss: 1.0968 - val_accuracy: 0.6228\n",
            "Epoch 38/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.5913 - accuracy: 0.7850 - val_loss: 1.0995 - val_accuracy: 0.6222\n",
            "Epoch 39/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.5643 - accuracy: 0.7960 - val_loss: 1.1030 - val_accuracy: 0.6230\n",
            "Epoch 40/50\n",
            "448/448 [==============================] - 18s 41ms/step - loss: 0.5505 - accuracy: 0.8008 - val_loss: 1.1127 - val_accuracy: 0.6268\n",
            "Epoch 41/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.5296 - accuracy: 0.8060 - val_loss: 1.1194 - val_accuracy: 0.6242\n",
            "Epoch 42/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.5113 - accuracy: 0.8132 - val_loss: 1.1466 - val_accuracy: 0.6226\n",
            "Epoch 43/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.5033 - accuracy: 0.8195 - val_loss: 1.1362 - val_accuracy: 0.6239\n",
            "Epoch 44/50\n",
            "448/448 [==============================] - 17s 38ms/step - loss: 0.4816 - accuracy: 0.8260 - val_loss: 1.1428 - val_accuracy: 0.6236\n",
            "Epoch 45/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.4610 - accuracy: 0.8315 - val_loss: 1.1475 - val_accuracy: 0.6272\n",
            "Epoch 46/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.4518 - accuracy: 0.8389 - val_loss: 1.1548 - val_accuracy: 0.6235\n",
            "Epoch 47/50\n",
            "448/448 [==============================] - 18s 39ms/step - loss: 0.4399 - accuracy: 0.8424 - val_loss: 1.1623 - val_accuracy: 0.6264\n",
            "Epoch 48/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.4270 - accuracy: 0.8449 - val_loss: 1.1751 - val_accuracy: 0.6247\n",
            "Epoch 49/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.4155 - accuracy: 0.8494 - val_loss: 1.1996 - val_accuracy: 0.6277\n",
            "Epoch 50/50\n",
            "448/448 [==============================] - 17s 39ms/step - loss: 0.3965 - accuracy: 0.8567 - val_loss: 1.1914 - val_accuracy: 0.6292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV_MsNPzIO38"
      },
      "source": [
        "emotion_model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OiIagP5IZe1"
      },
      "source": [
        "from keras.models import load_model\n",
        "emotion_model = load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkjxZWcIIZiT"
      },
      "source": [
        "def emotion_analysis(emotions):\n",
        "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "    y_pos = np.arange(len(objects))\n",
        "\n",
        "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
        "    plt.xticks(y_pos, objects)\n",
        "    plt.ylabel('percentage')\n",
        "    plt.title('emotion')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4NZ6opSIZlC"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYKQD3QIJQez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb8f6e63-de9c-4c33-89fa-bb3495f87875"
      },
      "source": [
        "take_photo()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'photo.jpg'"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTTion8LIZtz"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def facecrop(image):\n",
        "    facedata = '/content/haarcascade_frontalface_alt.xml'\n",
        "    cascade = cv2.CascadeClassifier(facedata)\n",
        "\n",
        "    img = cv2.imread(image)\n",
        "\n",
        "    try:\n",
        "\n",
        "        minisize = (img.shape[1],img.shape[0])\n",
        "        miniframe = cv2.resize(img, minisize)\n",
        "\n",
        "        faces = cascade.detectMultiScale(miniframe)\n",
        "\n",
        "        for f in faces:\n",
        "            x, y, w, h = [ v for v in f ]\n",
        "            cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        "\n",
        "            sub_face = img[y:y+h, x:x+w]\n",
        "\n",
        "\n",
        "            cv2.imwrite('capture.jpg', sub_face)\n",
        "            #print (\"Writing: \" + image)\n",
        "\n",
        "    except Exception as e:\n",
        "        print (e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    facecrop('/content/photo.jpg')\n",
        "\n",
        "#Testing a file.\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "file = '/content/photo.jpg'\n",
        "true_image = image.load_img(file)\n",
        "img = image.load_img(file, color_mode=\"grayscale\", target_size=(48, 48))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "x /= 255\n",
        "\n",
        "custom = emotion_model.predict(x)\n",
        "emotion_analysis(custom[0])\n",
        "\n",
        "x = np.array(x, 'float32')\n",
        "x = x.reshape([48, 48]);\n",
        "\n",
        "\n",
        "plt.imshow(true_image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}